#!/bin/bash


########################################
# ///                                        \\\
#  		You can edit your configuration here
#
#
########################################


massdnsWordlist=/root/Wordlist/subdomain/726w_subdomain.txt
ffuf_Wordlist=/root/Wordlist/endpoint/10w_common.txt





chromiumPath=/snap/bin/chromium
rootPath=/var/www/jsrecon/links
# rootPath=/root/OneDrive/output/lazyrecon

########################################
# Happy Hunting
########################################






red=`tput setaf 1`
green=`tput setaf 2`
yellow=`tput setaf 3`
reset=`tput sgr0`

SECONDS=0


usage() { echo -e "Usage: $0 -d domain [-e]\n  Select -e to specify excluded domains\n " 1>&2; exit 1; }

while getopts ":d:e:r:" o; do
    case "${o}" in
        d)
            domain=${OPTARG}
            ;;

            #### working on subdomain exclusion
        e)
            excluded=${OPTARG}
            ;;

        *)
            usage
            ;;
    esac
done
shift $((OPTIND - 1))




wayback(){

  waybackrecon
    waybackdowloader_oldjsfile
  checkjsfile


#   if [ ! -d "/var/www/jsrecon/links/$domain/wayback-data/jsfile/" ]; then
#   # Control will enter here if $DIRECTORY doesn't exist.
#     waybackdowloader_oldjsfile
#   checkjsfile
# fi



  echo "Using mento......."

  memento_mass -f $rootPath/$domain/$foldername/allsubdomains_final.txt  -o $rootPath/$domain/$foldername/useful/robots_archieve.txt

  # cp -r  $rootPath/$domain/$foldername/wayback-data/ /var/www/jsrecon/links/$domain/wayback-data/
  rm -rf $rootPath/$domain/$foldername/wayback-data/jsfile/
  cd $rootPath/$domain/$foldername/
}

discovery(){

  # #smuggler
  # cat $rootPath/$domain/$foldername/responsive_urls.txt | smuggler.py -l $rootPath/$domain/$foldername/useful/smuggler.txt
  
  wayback

  portscanning
  run_aquatone


}


waybackdowloader_oldjsfile(){
  cd $rootPath/$domain/$foldername/wayback-data/
  while read p ; do 
      wayback_machine_downloader $p  --only "/\.js$/i" > /dev/null
  done < $rootPath/$domain/$foldername/responsiveDomains.txt
  cd $rootPath/$domain/$foldername/wayback-data/websites/
  for fl_nm in $(find . -type f );
  do cp $fl_nm  $rootPath/$domain/$foldername/wayback-data/jsfile/$(echo $fl_nm | cut -c 3- |sed -e 's/[^A-Za-z0-9._-]/_/g'); 
done 

  rm -rf $rootPath/$domain/$foldername/wayback-data/websites/
}


find_firebase(){

  touch  $rootPath/$domain/$foldername/useful/firebase_js_location.txt

  cd $rootPath/$domain/$foldername/wayback-data/jsfile/
    for fl_nm in $(find . -type f );
  do grep "firebaseio" $fl_nm > /dev/null &&  readlink -f $fl_nm >> $rootPath/$domain/$foldername/useful/firebase_js_location.txt;
done 

if [ -s "$rootPath/$domain/$foldername/useful/firebase_js_location.txt" ]
  then python3  ~/recon_tools/Scripts/python/location2link.py $rootPath/$domain/$foldername/useful/firebase_js_location.txt $rootPath/$domain/$foldername/useful/firebase_js_link.html
  else
  rm $rootPath/$domain/$foldername/useful/firebase_js_location.txt
    return 
  fi

rm $rootPath/$domain/$foldername/useful/firebase_js_location.txt
cd -- 

    echo "  http://jsrecon.ragnarokv.site/links/${domain}/useful/firebase_js_link.html " | mutt -s "Found potential firebaseio url from  ${domain}"  inthebybyby@gmail.com   

}

checkjsfile(){
  cd $rootPath/$domain/$foldername/wayback-data/jsfile
  echo "Downloading js files using aria2c.............."
  aria2c -i $rootPath/$domain/$foldername/wayback-data/jsurls.txt > /dev/null
  # wget2 -i $rootPath/$domain/$foldername/wayback-data/jsurls.txt -P $rootPath/$domain/$foldername/wayback-data/jsfile
  fdupes . -r -f -1 -S -d -N 
  ls | while read file; do mv $file $(echo $file | sed -e 's/[^A-Za-z0-9._-]/_/g'); done
  # touch $rootPath/$domain/$foldername/useful/${domain}_secretfinder.html
  # SecretFinder.py -i './*' -o $rootPath/$domain/$foldername/useful/${domain}_secretfinder.html
  gf urls > $rootPath/$domain/useful/endpoints_injs.txt
  DumpsterDiver.py -p . -o  $rootPath/$domain/$foldername/useful/entropy.json > /dev/null
  python3 ~/recon_tools/Scripts/python/entropy_json.py $rootPath/$domain/$foldername/useful/entropy.json $rootPath/$domain/$foldername/useful/new.json

  # echo "Fininshed high entropy scanning  in jsfile http://jsrecon.ragnarokv.site/links/${domain}/useful/new.json " | mutt -s "Entropy string ${domain} "  inthebybyby@gmail.com

  find_firebase

  rm $rootPath/$domain/$foldername/wayback-data/jsfile/errors.log

echo "Generating tok......................."
cat * > $rootPath/$domain/$foldername/useless/merged.txt
cat $rootPath/$domain/$foldername/useless/merged.txt | tok |awk '!seen[$0]++' >  $rootPath/$domain/$foldername/useless/${domain}_word.txt
 comm -13 ~/Wordlist/other/rfc_words.txt $rootPath/$domain/$foldername/useless/${domain}_word.txt > $rootPath/$domain/$foldername/useful/${domain}_tok.txt
 rm $rootPath/$domain/$foldername/useless/merged.txt



  grep -oE ".{0,75}(application\/xml|encodeuricomponent|wsdl).{0,60}"  * > $rootPath/$domain/$foldername/useful/pattern.txt

  printf "\n\n\n\n Things from gf \n\n\n" >> $rootPath/$domain/$foldername/useful/pattern.txt
  gf http-auth  >> $rootPath/$domain/$foldername/useful/pattern.txt
  gf fw  >> $rootPath/$domain/$foldername/useful/pattern.txt
  gf firebase  >> $rootPath/$domain/$foldername/useful/pattern.txt
  gf s3-buckets  >> $rootPath/$domain/$foldername/useful/pattern.txt
  gf servers  >> $rootPath/$domain/$foldername/useful/pattern.txt
  gf sec  >> $rootPath/$domain/$foldername/useful/pattern.txt
  gf aws-keys  >> $rootPath/$domain/$foldername/useful/pattern.txt
  
  
echo "Using  linkfinder......................."

  python3  ~/recon_tools/LinkFinder/linkfinder.py -i './*'   -o $rootPath/$domain/$foldername/useful/${domain}_js.html

  python3 /root/recon_tools/Scripts/python/extract_link.py $rootPath/$domain/$foldername/useful/${domain}_js.html $rootPath/$domain/$foldername/useful/${domain}_other.html $rootPath/$domain/$foldername/useful/${domain}_api.html

  cd -


}

waybackrecon() {
echo "Scraping wayback for data..."
printf $domain  | gau -subs -providers  wayback | awk '!seen[$0]++'> $rootPath/$domain/$foldername/wayback-data/wayback.txt
printf $domain  | gau -subs -providers  commoncrawl | awk '!seen[$0]++' > $rootPath/$domain/$foldername/wayback-data/commoncrawl.txt
touch $rootPath/$domain/$foldername/wayback-data/temp.txt
cat $rootPath/$domain/$foldername/wayback-data/commoncrawl.txt >> $rootPath/$domain/$foldername/wayback-data/temp.txt
cat $rootPath/$domain/$foldername/wayback-data/wayback.txt >> $rootPath/$domain/$foldername/wayback-data/temp.txt
cat $rootPath/$domain/$foldername/wayback-data/temp.txt | awk '!seen[$0]++'>  $rootPath/$domain/$foldername/wayback-data/gau.txt

rm $rootPath/$domain/$foldername/wayback-data/temp.txt $rootPath/$domain/$foldername/wayback-data/commoncrawl.txt $rootPath/$domain/$foldername/wayback-data/wayback.txt

cat $rootPath/$domain/$foldername/wayback-data/gau.txt  | sort -u | grep -P "\w+\.js(\?|$)" | sort -u > $rootPath/$domain/$foldername/wayback-data/jsurls_temp.txt

cat $rootPath/$domain/$foldername/wayback-data/jsurls_temp.txt  | urldedeupe >  $rootPath/$domain/$foldername/wayback-data/jsurls.txt

[ -s $rootPath/$domain/$foldername/wayback-data/jsurls.txt ] && echo "JS Urls saved to $rootPath/$domain/$foldername/wayback-data/jsurls.txt"
rm $rootPath/$domain/$foldername/wayback-data/jsurls_temp.txt 

cat $rootPath/$domain/$foldername/wayback-data/gau.txt  |  unfurl -u paths >  $rootPath/$domain/$foldername/wayback-data/paths.txt
sed 's#/#\n#g' $rootPath/$domain/$foldername/wayback-data/paths.txt  | awk '!seen[$0]++' > $rootPath/$domain/$foldername/wayback-data/parts.txt





rscan.sh -f $rootPath/$domain/$foldername/wayback-data/gau.txt -o $rootPath/$domain/$foldername/wayback-data/



cat $rootPath/$domain/$foldername/wayback-data/output/params.txt >> $rootPath/$domain/$foldername/useful/finalwordlist.txt
# cat $rootPath/$domain/$foldername/wayback-data/output/wordlistgen.txt >> $rootPath/$domain/$foldername/useful/finalwordlist.txt


}

import_subdomains(){
    findomain --import-subdomains $rootPath/$domain/$foldername/allsubdomains_final.txt -m -t $domain --postgres-database subdomains_name
}

hostalive(){
echo "Probing for live hosts..."
cat $rootPath/$domain/$foldername/allsubdomains_final.txt  | httprobe -c 50 -t 3000 > $rootPath/$domain/$foldername/responsive_urls.txt

cat $rootPath/$domain/$foldername/responsive_urls.txt |unfurl -unique domain > $rootPath/$domain/$foldername/responsiveDomains.txt

run_subscraper

count=$(wc -l $rootPath/$domain/$foldername/responsiveDomains.txt | awk '{print $1}')

if [[ $count = 0 ]]; then
  exit 1
fi

echo  "${yellow}Total of ${count} live subdomains were found${reset}"
}


run_subscraper(){
     touch $rootPath/$domain/$foldername/useless/subscraper.txt
      timeout 2h  sh -c "cat $rootPath/$domain/$foldername/responsiveDomains.txt| parallel subscraper.py -u {} -o $rootPath/$domain/$foldername/useless/subscraper.txt > /dev/null"  
     cat $rootPath/$domain/$foldername/useless/subscraper.txt |  awk '!seen[$0]++' > $rootPath/$domain/$foldername/useless/unique_subscraper.txt
     cat $rootPath/$domain/$foldername/useless/unique_subscraper.txt |  httprobe -c 50 -t 3000 > $rootPath/$domain/$foldername/responsive_unique_subscraper_urls.txt
     cat $rootPath/$domain/$foldername/responsive_urls.txt >> $rootPath/$domain/$foldername/responsive_unique_subscraper_urls.txt
     cat $rootPath/$domain/$foldername/responsive_unique_subscraper_urls.txt |  awk '!seen[$0]++' > $rootPath/$domain/$foldername/responsive_urls.txt 
}

run_cloud_enum(){
brand=$(echo $domain |cut -d '.' -f 1 )
cloud_enum.py -k $domain -k $brand  -t 20 -l $rootPath/$domain/$foldername/useful/cloud_enum.txt

}



recon(){

  echo "${green}Recon started on $domain ${reset}"
  echo "Listing subdomains using assertfinder..."

  findomain -r -t $domain -u $rootPath/$domain/$foldername/raw_subdomains.txt  

  assetfinder --subs-only $domain >> $rootPath/$domain/$foldername/raw_subdomains.txt


  number=$(wc -l $rootPath/$domain/$foldername/raw_subdomains.txt| awk '{print $1}')
  if [[ $number -le 3 ]]; then
    ffuf -w $ffuf_Wordlist   -u https://${domain}/FUZZ -se -fs 0 -fw 1  > $rootPath/$domain/$foldername/${domain}_output.txt
    ip=$(dig +short $line @8.8.8.8| grep -m 1 -o '[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}')
    printf "${ip}" > $rootPath/$domain/$foldername/only_ip.txt
    ipport_mass -f  $rootPath/$domain/$foldername/only_ip.txt -o $rootPath/$domain/$foldername/portscanning
    echo "Fininshed ffuf scanning   " | mutt -s "FFUF_full ${domain}"  inthebybyby@gmail.com  -a $rootPath/$domain/$foldername/${domain}_output.txt
    exit 1
  fi




  if [[ "$(dig @1.1.1.1 A,CNAME {test321123,testingforwildcard,plsdontgimmearesult}.$domain +short | wc -l)" -gt "1" ]]; then
    echo "[!] Possible wildcard detected."
    echo "Skipping Massdns and dnsgen enumeration."
    wildcard_boolean=1
  fi


  echo "Starting Massdns Subdomain discovery this may take a while"
  mass 
  echo "Massdns finished..."

  echo "Using github-subdomains.py "
  github-subdomains.py -d $domain -t $github_token | tee  $rootPath/$domain/$foldername/useless/github_subdomain.txt

  cat $rootPath/$domain/$foldername/useless/github_subdomain.txt >> $rootPath/$domain/$foldername/raw_subdomains.txt

rm_dup_file $rootPath/$domain/$foldername/raw_subdomains.txt


  grep $domain  $rootPath/$domain/$foldername/raw_subdomains.txt | sed 's/*\.//'  >> $rootPath/$domain/$foldername/allsubdomains_final.txt

 
  hostalive

  # if [[ $wildcard_boolean !=  1 ]]; then
  #   #statements
  #   using_dnsgen
  #   hostalive_for_dnsgen
  # fi

  
  rm_dup_file $rootPath/$domain/$foldername/allsubdomains_final.txt
 

  import_subdomains



 
  cat $rootPath/$domain/$foldername/responsive_urls.txt  |unfurl -unique domain > $rootPath/$domain/$foldername/responsiveDomains_final.txt

  python3 /root/recon_tools/Scripts/python/urls_processing.py  $rootPath/$domain/$foldername/responsive_urls.txt  $rootPath/$domain/$foldername/responsive_urls.txt 




}


run_aquatone(){
  echo "Starting aquatone scan..."
  cd $rootPath/$domain/$foldername/aquatone
  cat $rootPath/$domain/$foldername/responsive_urls.txt | aquatone -chrome-path ~/other/chrome-linux/chrome
  cd -
	# cp -r  $rootPath/$domain/$foldername/aquatone /var/www/jsrecon/links/$domain/

}


using_blc(){
  touch $rootPath/$domain/$foldername/useful/blc.txt
  echo "Using blc "

  while read p ; do 
    timeout 5m blc  $p -rfoi --exclude youtube.com  --flter--level 3 >> $rootPath/$domain/$foldername/useful/blc.txt
  done < $rootPath/$domain/$foldername/responsive_urls.txt

  chmod +r $rootPath/$domain/$foldername/useful/blc.txt


}



portscanning(){
  touch $rootPath/$domain/$foldername/domain_ip_tmp.txt
  while read line; do
     ip=$(dig +short $line @8.8.8.8| grep -m 1 -o '[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}')
     if [[ -z $ip ]]; then
       continue
     fi
     echo "$line     $ip" >>$rootPath/$domain/$foldername/domain_ip_tmp.txt
  done <  $rootPath/$domain/$foldername/allsubdomains_final.txt

  cat $rootPath/$domain/$foldername/domain_ip_tmp.txt | awk '!seen[$2]++' > $rootPath/$domain/$foldername/domain_ip.txt

  rm $rootPath/$domain/$foldername/domain_ip_tmp.txt


  awk '{print $NF}' $rootPath/$domain/$foldername/domain_ip.txt | awk '!seen[$0]++' > $rootPath/$domain/$foldername/only_ip.txt

 timeout 3h ipport_mass -f  $rootPath/$domain/$foldername/only_ip.txt -o $rootPath/$domain/$foldername/portscanning

cp $rootPath/$domain/$foldername/portscanning/ip_services.txt $rootPath/$domain/$foldername/useful/ip_services.txt
}


mass(){

 if [[ $wildcard_boolean !=  1 ]]; then
   #statements
       /root/recon_tools/massdns/scripts/subbrute.py $massdnsWordlist $domain | massdns -r /root/Wordlist/resolver.txt -t A -q -o S > $rootPath/$domain/$foldername/useless/mass.txt



fi

deduplicate_massdns_output $rootPath/$domain/$foldername/useless/mass.txt

awk '{print $1}' $rootPath/$domain/$foldername/useless/mass.txt | sed 's/\.$//' > $rootPath/$domain/$foldername/massdns_subdomains.txt

rm_dup_file $rootPath/$domain/$foldername/massdns_subdomains.txt

cat $rootPath/$domain/$foldername/massdns_subdomains.txt >> $rootPath/$domain/$foldername/raw_subdomains.txt

}

deduplicate_massdns_output(){
tmpfile=/tmp/$(basename $1)
awk '!seen[$3]++' $1 > $tmpfile
mv $tmpfile $1
}

# using_dnsgen(){
#   echo "Using dnsgen..."

#   head -n 1500 $rootPath/$domain/$foldername/responsiveDomains.txt | dnsgen - > $rootPath/$domain/$foldername/useless/dnsgen_temp.txt

#   rm_dup_file  $rootPath/$domain/$foldername/useless/dnsgen_temp.txt

#   cat $rootPath/$domain/$foldername/useless/dnsgen_temp.txt | massdns -r /root/Wordlist/resolver.txt -t A -q -o S > $rootPath/$domain/$foldername/useless/dnsgen_mass.txt

#   deduplicate_massdns_output $rootPath/$domain/$foldername/useless/dnsgen_mass.txt

#   awk '{print $1}' $rootPath/$domain/$foldername/useless/dnsgen_mass.txt | sed 's/\.$//' > $rootPath/$domain/$foldername/useless/dnsgen_resolved_domains.txt

#     rm_dup_file  $rootPath/$domain/$foldername/useless/dnsgen_resolved_domains.txt

#   cat $rootPath/$domain/$foldername/useless/dnsgen_resolved_domains.txt >>  $rootPath/$domain/$foldername/allsubdomains_final.txt

#   rm $rootPath/$domain/$foldername/useless/dnsgen_temp.txt
# }

# hostalive_for_dnsgen(){
#   cat $rootPath/$domain/$foldername/useless/dnsgen_resolved_domains.txt | httprobe -c 50 -t 3000 > $rootPath/$domain/$foldername/useless/dnsgen_responsive.txt

#   cat $rootPath/$domain/$foldername/useless/dnsgen_responsive.txt >> $rootPath/$domain/$foldername/responsive_urls.txt

#   rm_dup_file $rootPath/$domain/$foldername/responsive_urls.txt




# }

nsrecords_subjack(){

                echo "${green}Started dns records check...${reset}"
                echo "Looking into CNAME Records..."


                touch $rootPath/$domain/$foldername/useful/pos.txt


                cat $rootPath/$domain/$foldername/useless/mass.txt | egrep 'CNAME|wp' > $rootPath/$domain/$foldername/useful/cnames.txt
                # cat $rootPath/$domain/$foldername/useless/dnsgen_mass.txt | grep 'CNAME|wp' >> $rootPath/$domain/$foldername/cnames.txt


                cat $rootPath/$domain/$foldername/cnames.txt | sort -u | while read line; do
                hostrec=$(echo "$line" | awk '{print $1}')
                if [[ $(host $hostrec | grep NXDOMAIN) != "" ]]
                then
                echo "${red}Check the following domain for NS takeover:  $line ${reset}"
                echo "$line" >> $rootPath/$domain/$foldername/useful/pos.txt
                else
                echo -ne "working on it...\r"
                fi
                done
                sleep 1

                subjack -c ~/go/src/github.com/haccer/subjack/fingerprints.json  -w $rootPath/$domain/$foldername/allsubdomains_final.txt  -t 10 -timeout 15 -o $rootPath/$domain/$foldername/useful/subjack.txt -ssl -a -m

                chmod +r $rootPath/$domain/$foldername/useful/subjack.txt


                echo "Fininshed subjack scanning  http://jsrecon.ragnarokv.site/links/${domain}/useful/subjack.txt " | mutt -s "Subjack ${domain} "  inthebybyby@gmail.com
        }



ffuf_403(){
  cd $rootPath/$domain/$foldername/
  mkdir $rootPath/$domain/$foldername/ffuf_403
  ffuf_403_process.py -i "ffuf/processed/*" "ffuf_paths/processed/*" -o $rootPath/$domain/$foldername/ffuf_403
  cp $rootPath/$domain/$foldername/ffuf_403/ffuf_403.html $rootPath/$domain/$foldername/ffuf_403/originalurl.html $rootPath/$domain/$foldername/ffuf_403/rewriteurls.html $rootPath/$domain/$foldername/useful

}

logo(){
  #can't have a bash script without a cool logo :D
  echo "${red}
 _     ____  ____ ___  _ ____  _____ ____  ____  _
/ \   /  _ \/_   \\\  \///  __\/  __//   _\/  _ \/ \  /|
| |   | / \| /   / \  / |  \/||  \  |  /  | / \|| |\ ||
| |_/\| |-||/   /_ / /  |    /|  /_ |  \__| \_/|| | \||
\____/\_/ \|\____//_/   \_/\_\\\____\\\____/\____/\_/  \\|
${reset}                                                      "
}


main(){
  logo


  mkdir -p $rootPath/$domain
  mkdir -p $rootPath/$domain/$foldername
  mkdir -p $rootPath/$domain/$foldername/useful
  mkdir -p $rootPath/$domain/$foldername/wayback-data/
  mkdir -p $rootPath/$domain/$foldername/wayback-data/jsfile

  mkdir -p $rootPath/$domain/$foldername/aquatone
  mkdir -p $rootPath/$domain/$foldername/useless
  mkdir -p $rootPath/$domain/$foldername/html
  mkdir -p /var/www/jsrecon/links/$domain


  touch $rootPath/$domain/$foldername/useless/mass.txt
  touch $rootPath/$domain/$foldername/cnames.txt
  touch $rootPath/$domain/$foldername/pos.txt
  touch $rootPath/$domain/$foldername/raw_subdomains.txt
  touch $rootPath/$domain/$foldername/useless/massdns_temp.txt
  touch $rootPath/$domain/$foldername/useless/cleantemp.txt
  touch $rootPath/$domain/$foldername/allsubdomains_final.txt
  touch $rootPath/$domain/$foldername/useful/finalwordlist.txt

  # rm_resolver
  recon
  nsrecords_subjack $domain
   run_cloud_enum
  # using_blc
  cat $rootPath/$domain/$foldername/responsive_urls.txt |  favfreak.py -o $rootPath/$domain/$foldername/useful/favfreak.txt --shodan

  echo "Using github-endpoints.py "

  github-endpoints.py -d $domain -s -r -t $github_token | tee $rootPath/$domain/$foldername/useful/github_endpoints.txt


  echo "Starting discovery..."
  discovery

 
  
  

  mkdir -p $rootPath/$domain/$foldername/ffuf/
  mkdir -p $rootPath/$domain/$foldername/ffuf_paths/

  ffuf_mass -f $rootPath/$domain/$foldername/responsive_urls.txt  -q -o $rootPath/$domain/$foldername/ffuf/ -w /root/Wordlist/endpoint/10w_common.txt
  ffuf_mass -f $rootPath/$domain/$foldername/responsive_urls.txt  -o $rootPath/$domain/$foldername/ffuf_paths/ -w $rootPath/$domain/$foldername/wayback-data/paths.txt
  cp $rootPath/$domain/$foldername/ffuf/ffuf_output.html $rootPath/$domain/$foldername/useful/ffuf_output.html
  cp $rootPath/$domain/$foldername/ffuf_paths/ffuf_output.html $rootPath/$domain/$foldername/useful/ffuf_paths_output.html
  echo "Fininshed ffuf scanning  http://jsrecon.ragnarokv.site/links/${domain}/useful/ffuf_output.html  http://jsrecon.ragnarokv.site/links/${domain}/useful/ffuf_output_paths.html  " | mutt -s "FFUF_full ${domain} "  inthebybyby@gmail.com


  # -a $rootPath/$domain/$foldername/ffuf/ffuf_output.html

  cat $rootPath/$domain/$foldername/useful/$domain_tok.txt >> $rootPath/$domain/$foldername/useful/finalwordlist.txt

  ffuf_403

gitround-runer.py -i $rootPath/$domain/$foldername/allsubdomains_final.txt  -n 5 -o $rootPath/$domain/$foldername/useful/gitrounds.txt


  # cp -r $rootPath/$domain/$foldername/useful /var/www/jsrecon/links/$domain/useful


  echo "${green}Scan for $domain finished successfully${reset}"
  duration=$SECONDS
  echo "Scan completed in : $(($duration / 60)) minutes and $(($duration % 60)) seconds."
  stty sane
  tput sgr0
}

todate=$(date +'%Y-%m-%d-%H-%M')
path=$(pwd)
foldername=recon-$todate

export rootPath
export domain
export foldername

touch /root/Wordlist/domains_monitoring.txt
echo $domain >> /root/Wordlist/domains_monitoring.txt

main $domain
